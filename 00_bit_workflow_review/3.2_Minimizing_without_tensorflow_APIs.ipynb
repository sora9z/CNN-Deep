{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1599286877763",
   "display_name": "Python 3.6.10 64-bit ('TF1': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2 Minimizing without tensorflow APIs\n",
    "\n",
    "## 목적 \n",
    "- tensorflow 함수를 사용하지 않고 Cost Minimizing 기능을 구현한다.\n",
    "- 이를 통하여 gradient descent에 대하여 이해한다.\n",
    "\n",
    "- Gradient Descent(경사하강법) : 손실함수를 최소화 하는 방법으로 parameter들은 업데이트 하는 학습 알고지름 중 대표적인 알고리즘.\n",
    "- 수식 : w(i)=w(i)-lr*(w에 대한 cost함수 편미분)\n",
    "- 경사하강법은 손실 함수가 최소가 되는 지점에서 종료하는 것이 가장 이상적 이지만,\n",
    "현실적으로 언제 손실함수가 최소가 될지 알기 어렵게 때문에 충분한 획수라고 생각되는 횟수만큼 업데이트를 진행한 후 학습을 종료함.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0 5.123857 [-0.04784036]\n1 1.4574524 [0.44115183]\n2 0.41456437 [0.7019476]\n3 0.11792046 [0.84103876]\n4 0.033541832 [0.9152207]\n5 0.009540772 [0.9547844]\n6 0.002713808 [0.97588503]\n7 0.00077193044 [0.9871387]\n8 0.00021957145 [0.99314064]\n9 6.245456e-05 [0.9963417]\n10 1.7764532e-05 [0.9980489]\n11 5.053326e-06 [0.9989594]\n12 1.4372741e-06 [0.999445]\n13 4.087949e-07 [0.999704]\n14 1.1635931e-07 [0.9998421]\n15 3.3111817e-08 [0.9999158]\n16 9.436334e-09 [0.99995506]\n17 2.6850036e-09 [0.99997604]\n18 7.5623063e-10 [0.99998724]\n19 2.1708975e-10 [0.9999932]\n20 6.2126304e-11 [0.99999636]\n"
    }
   ],
   "source": [
    "# Lab 3-2 Minimizing Cost\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(000)\n",
    "\n",
    "x_data=[1,2,3]\n",
    "y_data=[1,2,3]\n",
    "\n",
    "# Try to find values for W and b to compute y_data=X*W+b\n",
    "# We konow that W should be 1 and b 0 \n",
    "# But let's use Tensorflw to fugure it out\n",
    "\n",
    "W=tf.Variable(tf.random_normal([1]),name='Weight')\n",
    "X=tf.placeholder(tf.float32)\n",
    "Y=tf.placeholder(tf.float32)\n",
    "\n",
    "# Our hypothesis \n",
    "hypothesis=X*W\n",
    "\n",
    "# Cost/loss function\n",
    "cost=tf.reduce_mean(tf.square(hypothesis-Y))\n",
    "\n",
    "# Minimize Gradient Descent using derivative : W-=learning_rate*derivative\n",
    "learning_rate=0.1\n",
    "gradient=tf.reduce_mean((W*X-Y)*X)\n",
    "descent=W-learning_rate*gradient\n",
    "update=W.assign(descent)\n",
    "\n",
    "# Launch the graph in a session\n",
    "sess=tf.Session()\n",
    "\n",
    "# inintialize Variable \n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for step in range(21):\n",
    "    sess.run(update,feed_dict={X:x_data,Y:y_data})\n",
    "\n",
    "    print(step,sess.run(cost,feed_dict={X:x_data,Y:y_data}),sess.run(W))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}