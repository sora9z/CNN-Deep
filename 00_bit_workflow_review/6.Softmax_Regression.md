(** https://deepai.org/machine-learning-glossary-and-terms/softmax-layer 참고)
# 6. Sotfmax Rogistic Regression (Multinomial Logistic Regression)

## 1. What is Sotfmax Function?
- 입력받은 value를 출력으로 0~1 사이의 값으로 모두 정규화여 return하며 Probability 하고도 한다. 출력값들의 합은 항상 1이며 다중클레스 분류에 사용된다.
  
- Softmax function은 logistic regression에 사용되는 sigmoid function과 비슷며, class가 상호배타적(mutually exclusive, P(A or B)=P(A)+P(B)) 의 관계일 때 사용되는 classifier이기 때문에  multi-class logistic regression 이라도고 불린다. 
  
- multi-layer nn 의 output이 실수로 떨어지는 끝에서 두 번째의 layer의 경우 Softmax를 사용하여 정규화된 확률 분포로 변환하기 때문에 nn의 마지막 layer에 사용된다.
  
![](images/2020-10-09-17-43-14.png)</br></br>

- 예를들어 이미지 분류의 경우 이미지가 특정 class에 속할 확률은 다음과 같다.</br>
![](images/2020-10-09-17-46-38.png)

  
## 2. Mathematical definition of the Softmax Formula

![](images/2020-10-09-15-36-00.png)

- ## $\vec{z_{i}}$:</br>
  Input vector(z→)로, Z0~Zk로 구성되어있음.
- ## $z_{i}$ :</br>
   Input vector의 원소. 어떠한 실수,0,양수,음수의 값의 할당이 가능하다.</br>   ex) nn은 (-0.6,8.3,2.35,0) 과 같은 하나의 벡터를 output 갖을 수 있지만 확률분포의 값으로는 사용 불가능하므로 sotfmax를 사용하여 0-1사이의 값으로 정규화된 값으로 변형해야 한다.
- ## $e^{z_{i}}$ :</br>
   모든 원소에 대해 exponential function이 적용된다. 0상의 양의 값으로 반환되고 원소가 음의 값이라면 매우 작은 값이, 원소값이 매우 크면 큰 값이 반환된다.(아직 0-1사이의 값이 아님)
- ## $\sum_{i=1}^{K}{e^{z_{j}}}$ : </br>
  normalization term. 모든 output value가 1+0사이의 값을 정규화 하는 공식으로, 확률분포를 구성함.
  - ##**K** : </br>
    multi-class 에서 class의 수 

## 3. Calculating the Softmax
- 아래 input array늬 원소인 3개의  실수는 nn의 output이라 가정하고, 확률 분포로 전환해보자.
$A=\begin{bmatrix}
8\\
5\\
0\\
\end{bmatrix}
$
</br>
1) input array의 각 원소 exponential 계산한다. 이는 위의 softmx 수식의 분자값을 의미한다.</br>
$e^{z_{1}} = e^{8} = 2981.0$</br>
$e^{z_{2}} = e^{5} = 148.4 $</br>
$e^{z_{3}} = e^{0} = 1.0$</br>

- 아직 확률값이 아니며, 분모값(nomerization term)을 계산하여 나눠줌 으로써 확률분포 값으로 리턴한다.</br>

2) Normalization term</br></br>
$\sum_{i=1}^{K}{e^{z_{j}}}$ : </br>
$\sum_{i=1}^{K}{e^{z_{j}}} = e^{z_{1}} + e^{z_{2}} + e^{z_{3}} = 2981.0  + 148.4 + 1.0 = 3130.4$</br>

3) Normalization term을 각 원소의 exponential 값과 나눈다.
   ** sotfmax는 array를 array로 (서로 length가 같음.해당 array length=3)  변형하기 때문에 single value로 output을 하지 않는다.</br>

## $\sigma(\vec{z})_{1}$ = $2981.0 \over 3130.4$
 = 0.9523</br>
## $\sigma(\vec{z})_{2}$ = $148.4 \over 3130.4$ 
= 40.0474</br>
## $\sigma(\vec{z})_{3}$ = $1.0 \over 3130.4$ 
= 0.0003</br>

- output value의 전체 합은 1이여야 한다. </br>
** 첫 번째 값인 8의 exponential 값은 5와 0에 비해 매우 큰 값이기 때문에 5,0의 확률값은 매우 작다는 것은 중요한 포인트임.</br>
기계학습 모델에서 softmax 함수를 사용하는 경우, 0또는 1에 매우 가까운 값을 생성하는 경향이 있다. 위의 예와 같은 경우 실제 신경망 예측에 더 많은 불확실성이 있을 수 있는 첫 번째 class에 95% 확률을 할당했을 것임.**</br>

## 4. Softmax function in code
(1) Softmax hypothesis </br>
    - Python : softmax=exp(logits)/reduce_sum(exp(logits),dim)</br>
    - TF1 : hypothesis = tf.nn.softmax(tf.matmul(X,W)+b)</br>
    - TF2 : tf.model.add(tf.keras.layers.Activation('softmax'))</br>
    - Pytorch : softmax=torch.nn.Sorfmax()</br>
                model=torch.nn.Sequential(linear,softmax)</br>
                

# sotfmax VS sigmoid 비교한 것 정리하기 .









