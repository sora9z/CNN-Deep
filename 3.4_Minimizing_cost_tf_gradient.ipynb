{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1599289650465",
   "display_name": "Python 3.6.10 64-bit ('TF1': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.4 Minimizing Cost tf Gradient\n",
    "\n",
    "## 실습 목적\n",
    "- tf 함수를 사용한 것와 사용하지 않은 것을 비교한다.\n",
    "- 직접 편미분한 값과 내장함수를 사용한 함수를 비교함.\n",
    "\n",
    "- tf 함수를 사용\n",
    "    - minimize cost에서 gradient,W값을 얻음.\n",
    "    - gvs=optimizer.compute_gradient(cost,W)\n",
    "        - W변수의 cost에 대란 gradient 값을 얻어오는 함수.\n",
    "        - 얻은 gradient gvs를 변경하여 아래의 함수에 인자로 넣어서 결과를 수정할 수 있다.\n",
    "        - optimizer.apply_gradients(gvs)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0 [37.333332, 5.0, [(37.333336, 5.0)]]\n1 [2.4888866, 1.2666664, [(2.4888866, 1.2666664)]]\n2 [0.1659259, 1.0177778, [(0.1659259, 1.0177778)]]\n3 [0.011061668, 1.0011852, [(0.011061668, 1.0011852)]]\n4 [0.00073742867, 1.000079, [(0.00073742867, 1.000079)]]\n5 [4.895528e-05, 1.0000052, [(4.8955284e-05, 1.0000052)]]\n6 [3.0994415e-06, 1.0000004, [(3.0994415e-06, 1.0000004)]]\n7 [0.0, 1.0, [(0.0, 1.0)]]\n8 [0.0, 1.0, [(0.0, 1.0)]]\n9 [0.0, 1.0, [(0.0, 1.0)]]\n10 [0.0, 1.0, [(0.0, 1.0)]]\n11 [0.0, 1.0, [(0.0, 1.0)]]\n12 [0.0, 1.0, [(0.0, 1.0)]]\n13 [0.0, 1.0, [(0.0, 1.0)]]\n14 [0.0, 1.0, [(0.0, 1.0)]]\n15 [0.0, 1.0, [(0.0, 1.0)]]\n16 [0.0, 1.0, [(0.0, 1.0)]]\n17 [0.0, 1.0, [(0.0, 1.0)]]\n18 [0.0, 1.0, [(0.0, 1.0)]]\n19 [0.0, 1.0, [(0.0, 1.0)]]\n20 [0.0, 1.0, [(0.0, 1.0)]]\n21 [0.0, 1.0, [(0.0, 1.0)]]\n22 [0.0, 1.0, [(0.0, 1.0)]]\n23 [0.0, 1.0, [(0.0, 1.0)]]\n24 [0.0, 1.0, [(0.0, 1.0)]]\n25 [0.0, 1.0, [(0.0, 1.0)]]\n26 [0.0, 1.0, [(0.0, 1.0)]]\n27 [0.0, 1.0, [(0.0, 1.0)]]\n28 [0.0, 1.0, [(0.0, 1.0)]]\n29 [0.0, 1.0, [(0.0, 1.0)]]\n30 [0.0, 1.0, [(0.0, 1.0)]]\n31 [0.0, 1.0, [(0.0, 1.0)]]\n32 [0.0, 1.0, [(0.0, 1.0)]]\n33 [0.0, 1.0, [(0.0, 1.0)]]\n34 [0.0, 1.0, [(0.0, 1.0)]]\n35 [0.0, 1.0, [(0.0, 1.0)]]\n36 [0.0, 1.0, [(0.0, 1.0)]]\n37 [0.0, 1.0, [(0.0, 1.0)]]\n38 [0.0, 1.0, [(0.0, 1.0)]]\n39 [0.0, 1.0, [(0.0, 1.0)]]\n40 [0.0, 1.0, [(0.0, 1.0)]]\n41 [0.0, 1.0, [(0.0, 1.0)]]\n42 [0.0, 1.0, [(0.0, 1.0)]]\n43 [0.0, 1.0, [(0.0, 1.0)]]\n44 [0.0, 1.0, [(0.0, 1.0)]]\n45 [0.0, 1.0, [(0.0, 1.0)]]\n46 [0.0, 1.0, [(0.0, 1.0)]]\n47 [0.0, 1.0, [(0.0, 1.0)]]\n48 [0.0, 1.0, [(0.0, 1.0)]]\n49 [0.0, 1.0, [(0.0, 1.0)]]\n50 [0.0, 1.0, [(0.0, 1.0)]]\n51 [0.0, 1.0, [(0.0, 1.0)]]\n52 [0.0, 1.0, [(0.0, 1.0)]]\n53 [0.0, 1.0, [(0.0, 1.0)]]\n54 [0.0, 1.0, [(0.0, 1.0)]]\n55 [0.0, 1.0, [(0.0, 1.0)]]\n56 [0.0, 1.0, [(0.0, 1.0)]]\n57 [0.0, 1.0, [(0.0, 1.0)]]\n58 [0.0, 1.0, [(0.0, 1.0)]]\n59 [0.0, 1.0, [(0.0, 1.0)]]\n60 [0.0, 1.0, [(0.0, 1.0)]]\n61 [0.0, 1.0, [(0.0, 1.0)]]\n62 [0.0, 1.0, [(0.0, 1.0)]]\n63 [0.0, 1.0, [(0.0, 1.0)]]\n64 [0.0, 1.0, [(0.0, 1.0)]]\n65 [0.0, 1.0, [(0.0, 1.0)]]\n66 [0.0, 1.0, [(0.0, 1.0)]]\n67 [0.0, 1.0, [(0.0, 1.0)]]\n68 [0.0, 1.0, [(0.0, 1.0)]]\n69 [0.0, 1.0, [(0.0, 1.0)]]\n70 [0.0, 1.0, [(0.0, 1.0)]]\n71 [0.0, 1.0, [(0.0, 1.0)]]\n72 [0.0, 1.0, [(0.0, 1.0)]]\n73 [0.0, 1.0, [(0.0, 1.0)]]\n74 [0.0, 1.0, [(0.0, 1.0)]]\n75 [0.0, 1.0, [(0.0, 1.0)]]\n76 [0.0, 1.0, [(0.0, 1.0)]]\n77 [0.0, 1.0, [(0.0, 1.0)]]\n78 [0.0, 1.0, [(0.0, 1.0)]]\n79 [0.0, 1.0, [(0.0, 1.0)]]\n80 [0.0, 1.0, [(0.0, 1.0)]]\n81 [0.0, 1.0, [(0.0, 1.0)]]\n82 [0.0, 1.0, [(0.0, 1.0)]]\n83 [0.0, 1.0, [(0.0, 1.0)]]\n84 [0.0, 1.0, [(0.0, 1.0)]]\n85 [0.0, 1.0, [(0.0, 1.0)]]\n86 [0.0, 1.0, [(0.0, 1.0)]]\n87 [0.0, 1.0, [(0.0, 1.0)]]\n88 [0.0, 1.0, [(0.0, 1.0)]]\n89 [0.0, 1.0, [(0.0, 1.0)]]\n90 [0.0, 1.0, [(0.0, 1.0)]]\n91 [0.0, 1.0, [(0.0, 1.0)]]\n92 [0.0, 1.0, [(0.0, 1.0)]]\n93 [0.0, 1.0, [(0.0, 1.0)]]\n94 [0.0, 1.0, [(0.0, 1.0)]]\n95 [0.0, 1.0, [(0.0, 1.0)]]\n96 [0.0, 1.0, [(0.0, 1.0)]]\n97 [0.0, 1.0, [(0.0, 1.0)]]\n98 [0.0, 1.0, [(0.0, 1.0)]]\n99 [0.0, 1.0, [(0.0, 1.0)]]\n"
    }
   ],
   "source": [
    "# Lab 3-4 Minimizing cost tf gradient\n",
    "# This is optional\n",
    "#\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(00)\n",
    "\n",
    "# tf Graph input\n",
    "X=[1,2,3]\n",
    "Y=[1,2,3]\n",
    "\n",
    "# Set Wrong model weights\n",
    "W=tf.Variable(5.)\n",
    "\n",
    "# Linear Model\n",
    "hypothesis=W*X\n",
    "\n",
    "# Manual Gradient\n",
    "gradient=tf.reduce_mean((W*X-Y)*X)*2\n",
    "\n",
    "# cost \n",
    "cost=tf.reduce_mean(tf.square(hypothesis-Y))\n",
    "\n",
    "# Minimize : Gradient Descent\n",
    "optimizer=tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "train=optimizer.minimize(cost)\n",
    "\n",
    "#Get Gradient\n",
    "gvs=optimizer.compute_gradients(cost,[W]) # W변수의 cost에 대한 gradient 값을 얻어오는 함수. 미분값들을 Graph로 보기 위하여. \n",
    "gvs2=optimizer.compute_gradients(cost,W) #[W] 했을때랑 무엇이 다른지. 똑같으데 ????\n",
    "\n",
    "# Optional : modify gradient necessary\n",
    "# gvs=[(tf.clip_by_value(grad,-1.,1.),var) for grad,var in gvs]\n",
    "# Clips tensor values to a specified min and max.\n",
    "# tf.clib_by_value(t,min,max,name) : t: A Tensor or IndexedSlices.\n",
    "\n",
    "# Apply gradient\n",
    "\n",
    "apply_gradients=optimizer.apply_gradients(gvs) #gradient gvs 값을 인자로 사용하여 W값을 얻기 위한 그래프를 생성.and\n",
    "\n",
    "# Launch the grap in a session\n",
    "sess=tf.Session()\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for step in range(100):\n",
    "    print(step,sess.run([gradient,W,gvs]))\n",
    "    sess.run(apply_gradients) # Same as sess.run(train)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Manual Grad: 0 28.0 2.2\nAuto Grad: 0 [(28.0, 5.0)]\nManual Grad: 1 1.8666676 2.0133333\nAuto Grad: 1 [(1.8666676, 2.2)]\nManual Grad: 2 0.12444433 2.0008888\nAuto Grad: 2 [(0.12444433, 2.0133333)]\nManual Grad: 3 0.008295695 2.0000594\nAuto Grad: 3 [(0.008295695, 2.0008888)]\nManual Grad: 4 0.0005545616 2.0000038\nAuto Grad: 4 [(0.0005545616, 2.0000594)]\nManual Grad: 5 3.560384e-05 2.0000002\nAuto Grad: 5 [(3.5603844e-05, 2.0000038)]\nManual Grad: 6 2.7020772e-06 2.0\nAuto Grad: 6 [(2.7020774e-06, 2.0000002)]\nManual Grad: 7 0.0 2.0\nAuto Grad: 7 [(0.0, 2.0)]\nManual Grad: 8 0.0 2.0\nAuto Grad: 8 [(0.0, 2.0)]\nManual Grad: 9 0.0 2.0\nAuto Grad: 9 [(0.0, 2.0)]\nManual Grad: 10 0.0 2.0\nAuto Grad: 10 [(0.0, 2.0)]\nManual Grad: 11 0.0 2.0\nAuto Grad: 11 [(0.0, 2.0)]\nManual Grad: 12 0.0 2.0\nAuto Grad: 12 [(0.0, 2.0)]\nManual Grad: 13 0.0 2.0\nAuto Grad: 13 [(0.0, 2.0)]\nManual Grad: 14 0.0 2.0\nAuto Grad: 14 [(0.0, 2.0)]\nManual Grad: 15 0.0 2.0\nAuto Grad: 15 [(0.0, 2.0)]\nManual Grad: 16 0.0 2.0\nAuto Grad: 16 [(0.0, 2.0)]\nManual Grad: 17 0.0 2.0\nAuto Grad: 17 [(0.0, 2.0)]\nManual Grad: 18 0.0 2.0\nAuto Grad: 18 [(0.0, 2.0)]\nManual Grad: 19 0.0 2.0\nAuto Grad: 19 [(0.0, 2.0)]\nManual Grad: 20 0.0 2.0\nAuto Grad: 20 [(0.0, 2.0)]\nManual Grad: 21 0.0 2.0\nAuto Grad: 21 [(0.0, 2.0)]\nManual Grad: 22 0.0 2.0\nAuto Grad: 22 [(0.0, 2.0)]\nManual Grad: 23 0.0 2.0\nAuto Grad: 23 [(0.0, 2.0)]\nManual Grad: 24 0.0 2.0\nAuto Grad: 24 [(0.0, 2.0)]\nManual Grad: 25 0.0 2.0\nAuto Grad: 25 [(0.0, 2.0)]\nManual Grad: 26 0.0 2.0\nAuto Grad: 26 [(0.0, 2.0)]\nManual Grad: 27 0.0 2.0\nAuto Grad: 27 [(0.0, 2.0)]\nManual Grad: 28 0.0 2.0\nAuto Grad: 28 [(0.0, 2.0)]\nManual Grad: 29 0.0 2.0\nAuto Grad: 29 [(0.0, 2.0)]\nManual Grad: 30 0.0 2.0\nAuto Grad: 30 [(0.0, 2.0)]\nManual Grad: 31 0.0 2.0\nAuto Grad: 31 [(0.0, 2.0)]\nManual Grad: 32 0.0 2.0\nAuto Grad: 32 [(0.0, 2.0)]\nManual Grad: 33 0.0 2.0\nAuto Grad: 33 [(0.0, 2.0)]\nManual Grad: 34 0.0 2.0\nAuto Grad: 34 [(0.0, 2.0)]\nManual Grad: 35 0.0 2.0\nAuto Grad: 35 [(0.0, 2.0)]\nManual Grad: 36 0.0 2.0\nAuto Grad: 36 [(0.0, 2.0)]\nManual Grad: 37 0.0 2.0\nAuto Grad: 37 [(0.0, 2.0)]\nManual Grad: 38 0.0 2.0\nAuto Grad: 38 [(0.0, 2.0)]\nManual Grad: 39 0.0 2.0\nAuto Grad: 39 [(0.0, 2.0)]\nManual Grad: 40 0.0 2.0\nAuto Grad: 40 [(0.0, 2.0)]\nManual Grad: 41 0.0 2.0\nAuto Grad: 41 [(0.0, 2.0)]\nManual Grad: 42 0.0 2.0\nAuto Grad: 42 [(0.0, 2.0)]\nManual Grad: 43 0.0 2.0\nAuto Grad: 43 [(0.0, 2.0)]\nManual Grad: 44 0.0 2.0\nAuto Grad: 44 [(0.0, 2.0)]\nManual Grad: 45 0.0 2.0\nAuto Grad: 45 [(0.0, 2.0)]\nManual Grad: 46 0.0 2.0\nAuto Grad: 46 [(0.0, 2.0)]\nManual Grad: 47 0.0 2.0\nAuto Grad: 47 [(0.0, 2.0)]\nManual Grad: 48 0.0 2.0\nAuto Grad: 48 [(0.0, 2.0)]\nManual Grad: 49 0.0 2.0\nAuto Grad: 49 [(0.0, 2.0)]\nManual Grad: 50 0.0 2.0\nAuto Grad: 50 [(0.0, 2.0)]\nManual Grad: 51 0.0 2.0\nAuto Grad: 51 [(0.0, 2.0)]\nManual Grad: 52 0.0 2.0\nAuto Grad: 52 [(0.0, 2.0)]\nManual Grad: 53 0.0 2.0\nAuto Grad: 53 [(0.0, 2.0)]\nManual Grad: 54 0.0 2.0\nAuto Grad: 54 [(0.0, 2.0)]\nManual Grad: 55 0.0 2.0\nAuto Grad: 55 [(0.0, 2.0)]\nManual Grad: 56 0.0 2.0\nAuto Grad: 56 [(0.0, 2.0)]\nManual Grad: 57 0.0 2.0\nAuto Grad: 57 [(0.0, 2.0)]\nManual Grad: 58 0.0 2.0\nAuto Grad: 58 [(0.0, 2.0)]\nManual Grad: 59 0.0 2.0\nAuto Grad: 59 [(0.0, 2.0)]\nManual Grad: 60 0.0 2.0\nAuto Grad: 60 [(0.0, 2.0)]\nManual Grad: 61 0.0 2.0\nAuto Grad: 61 [(0.0, 2.0)]\nManual Grad: 62 0.0 2.0\nAuto Grad: 62 [(0.0, 2.0)]\nManual Grad: 63 0.0 2.0\nAuto Grad: 63 [(0.0, 2.0)]\nManual Grad: 64 0.0 2.0\nAuto Grad: 64 [(0.0, 2.0)]\nManual Grad: 65 0.0 2.0\nAuto Grad: 65 [(0.0, 2.0)]\nManual Grad: 66 0.0 2.0\nAuto Grad: 66 [(0.0, 2.0)]\nManual Grad: 67 0.0 2.0\nAuto Grad: 67 [(0.0, 2.0)]\nManual Grad: 68 0.0 2.0\nAuto Grad: 68 [(0.0, 2.0)]\nManual Grad: 69 0.0 2.0\nAuto Grad: 69 [(0.0, 2.0)]\nManual Grad: 70 0.0 2.0\nAuto Grad: 70 [(0.0, 2.0)]\nManual Grad: 71 0.0 2.0\nAuto Grad: 71 [(0.0, 2.0)]\nManual Grad: 72 0.0 2.0\nAuto Grad: 72 [(0.0, 2.0)]\nManual Grad: 73 0.0 2.0\nAuto Grad: 73 [(0.0, 2.0)]\nManual Grad: 74 0.0 2.0\nAuto Grad: 74 [(0.0, 2.0)]\nManual Grad: 75 0.0 2.0\nAuto Grad: 75 [(0.0, 2.0)]\nManual Grad: 76 0.0 2.0\nAuto Grad: 76 [(0.0, 2.0)]\nManual Grad: 77 0.0 2.0\nAuto Grad: 77 [(0.0, 2.0)]\nManual Grad: 78 0.0 2.0\nAuto Grad: 78 [(0.0, 2.0)]\nManual Grad: 79 0.0 2.0\nAuto Grad: 79 [(0.0, 2.0)]\nManual Grad: 80 0.0 2.0\nAuto Grad: 80 [(0.0, 2.0)]\nManual Grad: 81 0.0 2.0\nAuto Grad: 81 [(0.0, 2.0)]\nManual Grad: 82 0.0 2.0\nAuto Grad: 82 [(0.0, 2.0)]\nManual Grad: 83 0.0 2.0\nAuto Grad: 83 [(0.0, 2.0)]\nManual Grad: 84 0.0 2.0\nAuto Grad: 84 [(0.0, 2.0)]\nManual Grad: 85 0.0 2.0\nAuto Grad: 85 [(0.0, 2.0)]\nManual Grad: 86 0.0 2.0\nAuto Grad: 86 [(0.0, 2.0)]\nManual Grad: 87 0.0 2.0\nAuto Grad: 87 [(0.0, 2.0)]\nManual Grad: 88 0.0 2.0\nAuto Grad: 88 [(0.0, 2.0)]\nManual Grad: 89 0.0 2.0\nAuto Grad: 89 [(0.0, 2.0)]\nManual Grad: 90 0.0 2.0\nAuto Grad: 90 [(0.0, 2.0)]\nManual Grad: 91 0.0 2.0\nAuto Grad: 91 [(0.0, 2.0)]\nManual Grad: 92 0.0 2.0\nAuto Grad: 92 [(0.0, 2.0)]\nManual Grad: 93 0.0 2.0\nAuto Grad: 93 [(0.0, 2.0)]\nManual Grad: 94 0.0 2.0\nAuto Grad: 94 [(0.0, 2.0)]\nManual Grad: 95 0.0 2.0\nAuto Grad: 95 [(0.0, 2.0)]\nManual Grad: 96 0.0 2.0\nAuto Grad: 96 [(0.0, 2.0)]\nManual Grad: 97 0.0 2.0\nAuto Grad: 97 [(0.0, 2.0)]\nManual Grad: 98 0.0 2.0\nAuto Grad: 98 [(0.0, 2.0)]\nManual Grad: 99 0.0 2.0\nAuto Grad: 99 [(0.0, 2.0)]\n"
    }
   ],
   "source": [
    "## 위의 코드는 내장함수를 사용하는 것과 직접 편미분 한것의 비교를 하지 못하는 것 같음.\n",
    "## 다시 코드를 작성한다.\n",
    "## Manual gradient 와  Auto gradient 를 직접 구하고 \n",
    "#3 Graph로 비교한다.\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tf.set_random_seed(000)\n",
    "\n",
    "X=[1,2,3]\n",
    "Y=[2,4,6]\n",
    "\n",
    "\n",
    "W1=tf.Variable(5.,name='weight1') # Not Using tf function\n",
    "W2=tf.Variable(5.,name='Weight2') # Using tf function W2=tf.Variable(tf.random_normal([1]),name='weight1') 이렇게 하면 타입? 형식으로 결과가 나온다\n",
    "\n",
    "# Do not use tf function : Manual gradient\n",
    "\n",
    "gradient=tf.reduce_mean((W1*X-Y)*X)*2\n",
    "lr=0.1\n",
    "descent=W1-lr*gradient\n",
    "update=W1.assign(descent)\n",
    "\n",
    "# Use tf function : Auto Gradient\n",
    "hypothesis=W2*X\n",
    "cost=tf.reduce_mean(tf.square(hypothesis-Y))\n",
    "optimizer=tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "train=optimizer.minimize(cost)\n",
    "\n",
    "## Get gradient \n",
    "gvs=optimizer.compute_gradients(cost,[W2])\n",
    "apply_gradients=optimizer.apply_gradients(gvs)\n",
    "\n",
    "\n",
    "# Launch the graph in a session\n",
    "sess=tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for step in range(100):\n",
    "    print('Manual Grad:',step,sess.run(gradient),sess.run(update))\n",
    "    print('Auto Grad:',step,sess.run(gvs))\n",
    "    sess.run(apply_gradients)\n",
    "\n",
    "\n"
   ]
  }
 ]
}