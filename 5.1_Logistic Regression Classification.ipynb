{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1599998653452",
   "display_name": "Python 3.6.10 64-bit ('TF1': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Logistic Regression Classification\n",
    "\n",
    "#(1) Logistic Regression ?\n",
    "\n",
    "    - 이진분류 (binary classification) : 0 또는 1의 결과값을 갖는 분류에 사용\n",
    "    - Hypothesis 로 sigmoid 함수르 사용한다.\n",
    "\n",
    "#(2) Sigmoid Function\n",
    "   \n",
    "    - Hypothesis(x)=1/(1+exp(x))^-y\n",
    "\n",
    "#(3) Cost Function\n",
    "\n",
    "    - cost(H(x),y) \n",
    "      \n",
    "        - y=1 : -log(H(x))\n",
    "        - y=-0: -log(1-H(x))\n",
    "        - total cost(W)=1/m sum(cost(H(x),y))\n",
    "\n",
    "#(4) Gradient Descent Algorithm\n",
    "\n",
    "    - cost(W)=-1/m sum(ylog(H(x))+(1-y)log(1-H(x)))\n",
    "    - W:=W-lr*dcost(W)/dW\n",
    "\n",
    "#(5) cost Function\n",
    "\n",
    "    - cost=tf.reduce_mean(-tf.reduce_sum(Y*tf.log(hypothesis)+(1-Y)*tf.log(1-hypothesis)))\n",
    "\n",
    "#(6) Minimize\n",
    "\n",
    "    - a=tf.Variable(0.1) #learning rate\n",
    "    - optimiazer=tf.train.GradientDescentOptimizer(a)\n",
    "    - train=optimizer(cost)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------------------------------------\n",
    "#(1) 실습 목적\n",
    "    - 기본적인 텐서플로우 동작 시키기 위한 구조를 파악한다.\n",
    "    - 이를 위한 텐서플로우 기능을 익힌다\n",
    "\n",
    "#(2) 사용한 API 및 간단한 설명\n",
    "\n",
    "    - hypothesis=tf.sigmoid(tf.matmul(X,W)+b)\n",
    "        - hypothesis에서 tf.matmul(X,W)+b에 sigmoid함수 적용\n",
    "\n",
    "    - cost=tf.reduce_mean(Y*tf.log(hypothesis)+(1-Y)*tf.log(1-hypothesis))\n",
    "        - Cross entroph를 코스츠 함수로 사용한다.\n",
    "        - 1인 경우 Y*tf.log(hypothesis)사 0으로 수렴하도록 학습\n",
    "        - 0인 경우 (1-Y)*log(1-hypothesis)가 0으로 수렴하도록 학습.\n",
    "        - reduce_mean\n",
    "    \n",
    "    Training후 test 진행\n",
    "    - predicted=tf.cast(hypothesis>0.5,dtype=tf.float32)\n",
    "        - 0.5보다 작으면 0, 0.5보다 크면 1로 predicted 변수를 설정\n",
    "        - predicted의 데이터 다입을 float32로 설정\n",
    "        - logistic regression은 0과 1로 구분하기 때문에, 0.5를 기준으로 구분한다.\n",
    "        - cast\n",
    "\n",
    "    - accuracy =tf.reduce_mean(tf.cast(tf.equal(predictged,Y),dtype=tf.float32))\n",
    "        - tf.equal(predicted,Y) 가 같으면 1다르면 0 이를 더해서 평균을 낸다.\n",
    "        - 정답과 결과를 비교하여 정확도를 측정한다.\n",
    "        - reduce_mean  equal \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0 2.0317173\n200 0.6088313\n400 0.48601094\n600 0.43186125\n800 0.4020289\n1000 0.38199183\n1200 0.36649808\n1400 0.35340813\n1600 0.34176067\n1800 0.33108684\n2000 0.3211417\n2200 0.31178713\n2400 0.30293944\n2600 0.29454312\n2800 0.28655836\n3000 0.2789541\n3200 0.27170444\n3400 0.26478717\n3600 0.25818226\n3800 0.25187117\n4000 0.24583709\n4200 0.24006444\n4400 0.23453838\n4600 0.22924511\n4800 0.22417171\n5000 0.21930616\n5200 0.21463709\n5400 0.2101539\n5600 0.20584656\n5800 0.20170583\n6000 0.19772285\n6200 0.19388938\n6400 0.19019775\n6600 0.1866407\n6800 0.1832115\n70000.17990363\n7200 0.17671122\n7400 0.17362861\n7600 0.17065036\n7800 0.16777171\n8000 0.16498774\n8200 0.16229422\n8400 0.1596868\n8600 0.1571618\n8800 0.15471525\n9000 0.15234388\n9200 0.1500442\n9400 0.14781325\n9600 0.14564796\n9800 0.14354561\n10000 0.14150353\n\nHypothesis: [[0.02745163]\n [0.15413675]\n [0.28872007]\n [0.7888586 ]\n [0.94418854]\n [0.9817281 ]] \nPredicted [[0.]\n [0.]\n [0.]\n [1.]\n [1.]\n [1.]] \nAccuracy 1.0\n"
    }
   ],
   "source": [
    "# Lab 5-1 Logistic Regression Classification\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(555)\n",
    "\n",
    "x_data=[[1,2],[2,3],[3,1],[4,3],[5,3],[6,2]]\n",
    "y_data=[[0],[0],[0],[1],[1],[1]]\n",
    "\n",
    "# Placeholder for a tensor that will be always fed\n",
    "X=tf.placeholder(tf.float32,shape=[None,2])\n",
    "Y=tf.placeholder(tf.float32,shape=[None,1])\n",
    "\n",
    "W=tf.Variable(tf.random_normal([2,1]),name='weight')\n",
    "b=tf.Variable(tf.random_normal([1]),name='bias')\n",
    "\n",
    "#Hypothesis using sigmoid : tf.div(1.,tf.exp(tf.matmul(X,W)))\n",
    "hypothesis=tf.sigmoid(tf.matmul(X,W)+b)\n",
    "\n",
    "#cost/loss functing\n",
    "cost=-tf.reduce_mean((Y*tf.log(hypothesis)+(1-Y)*tf.log(1-hypothesis)))\n",
    "\n",
    "train=tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
    "\n",
    "#Accuracy computaion\n",
    "#True if hypothesis>0.5 else False\n",
    "predicted=tf.cast(hypothesis>0.5,dtype=tf.float32)\n",
    "accuracy=tf.reduce_mean(tf.cast(tf.equal(predicted,Y),dtype=tf.float32))\n",
    "\n",
    "# Launch graph\n",
    "# Initialize Tensorflow variables\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #print(\"1.Check for changes the cost \")\n",
    "    \n",
    "    for step in range(10001):\n",
    "        cost_val,_=sess.run([cost,train],feed_dict={X:x_data,Y:y_data})\n",
    "        if step%200==0:\n",
    "            print(step,cost_val)\n",
    "    #Accuracy report\n",
    "    h,c,a=sess.run([hypothesis,predicted,accuracy],feed_dict={X:x_data,Y:y_data})\n",
    "    print(\"\\nHypothesis:\",h,\"\\nPredicted\",c,\"\\nAccuracy\",a)\n",
    "    "
   ]
  }
 ]
}