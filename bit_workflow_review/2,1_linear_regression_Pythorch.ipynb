{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1598872803124",
   "display_name": "Python 3.6.10 64-bit ('TF2': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 Linear Regression by Pytorch\n",
    "\n",
    "참고 :\n",
    "https://tutorials.pytorch.kr/beginner/pytorch_with_examples.html\n",
    "\n",
    "- # Pytorch 만의 문법 체계\n",
    "     # 1 hypothesis = model(X) ; model 은 nn.Linear.\n",
    "     # 2 cost function = criterion.\n",
    "     # 3 기울기 초기화 ; optimizer.zero_grad() ; 기울기 값을 0으로 초기화한다.\n",
    "     # 4 편미분 ; cost.backward().\n",
    "     # 5 weight, bias update ; optimizer.step().\n",
    "     \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0 \ncost: 0.31168637 \nweight: [[0.4511068]] \nbias: [0.8341803]\n20 \ncost: 0.108099826 \nweight: [[0.607661]] \nbias: [0.8524978]\n40 \ncost: 0.09659154 \nweight: [[0.63855004]] \nbias: [0.8179106]\n60 \ncost: 0.0877116 \nweight: [[0.6567224]] \nbias: [0.7799934]\n80 \ncost: 0.079660915 \nweight: [[0.6729681]] \nbias: [0.7433862]\n100 \ncost: 0.07234932 \nweight: [[0.68834823]] \nbias: [0.7084544]\n120 \ncost: 0.06570878 \nweight: [[0.70299584]] \nbias: [0.6751601]\n140 \ncost: 0.05967778 \nweight: [[0.71695405]] \nbias: [0.64343005]\n160 \ncost: 0.05420031 \nweight: [[0.73025614]] \nbias: [0.61319125]\n180 \ncost: 0.04922558 \nweight: [[0.7429331]] \nbias: [0.58437353]\n200 \ncost: 0.04470745 \nweight: [[0.75501424]] \nbias: [0.55691004]\n220 \ncost: 0.04060403 \nweight: [[0.7665278]] \nbias: [0.5307373]\n240 \ncost: 0.036877245 \nweight: [[0.77750003]] \nbias: [0.50579464]\n260 \ncost: 0.0334925 \nweight: [[0.7879567]] \nbias: [0.4820242]\n280 \ncost: 0.030418439 \nweight: [[0.797922]] \nbias: [0.45937076]\n300 \ncost: 0.0276265 \nweight: [[0.8074189]] \nbias: [0.43778205]\n320 \ncost: 0.025090808 \nweight: [[0.8164696]] \nbias: [0.4172079]\n340 \ncost: 0.022787884 \nweight: [[0.82509476]] \nbias: [0.3976006]\n360 \ncost: 0.020696301 \nweight: [[0.8333147]] \nbias: [0.37891483]\n380 \ncost: 0.018796733 \nweight: [[0.8411483]] \nbias: [0.36110726]\n400 \ncost: 0.017071487 \nweight: [[0.84861374]] \nbias: [0.34413654]\n420 \ncost: 0.015504599 \nweight: [[0.8557284]] \nbias: [0.32796335]\n440 \ncost: 0.014081523 \nweight: [[0.86250865]] \nbias: [0.31255025]\n460 \ncost: 0.0127890585 \nweight: [[0.8689702]] \nbias: [0.29786152]\n480 \ncost: 0.011615234 \nweight: [[0.8751281]] \nbias: [0.28386316]\n500 \ncost: 0.010549148 \nweight: [[0.8809966]] \nbias: [0.27052268]\n520 \ncost: 0.009580896 \nweight: [[0.8865893]] \nbias: [0.2578091]\n540 \ncost: 0.008701533 \nweight: [[0.89191926]] \nbias: [0.24569303]\n560 \ncost: 0.007902871 \nweight: [[0.8969986]] \nbias: [0.23414636]\n580 \ncost: 0.0071775108 \nweight: [[0.9018393]] \nbias: [0.22314236]\n600 \ncost: 0.0065187407 \nweight: [[0.9064525]] \nbias: [0.2126555]\n620 \ncost: 0.005920405 \nweight: [[0.9108489]] \nbias: [0.20266144]\n640 \ncost: 0.0053770156 \nweight: [[0.91503865]] \nbias: [0.1931371]\n660 \ncost: 0.004883498 \nweight: [[0.91903144]] \nbias: [0.18406036]\n680 \ncost: 0.0044352617 \nweight: [[0.9228368]] \nbias: [0.17541021]\n700 \ncost: 0.004028184 \nweight: [[0.9264631]] \nbias: [0.16716658]\n720 \ncost: 0.0036584542 \nweight: [[0.92991906]] \nbias: [0.15931042]\n740 \ncost: 0.0033226672 \nweight: [[0.93321264]] \nbias: [0.15182345]\n760 \ncost: 0.003017704 \nweight: [[0.93635136]] \nbias: [0.14468828]\n780 \ncost: 0.0027407233 \nweight: [[0.93934256]] \nbias: [0.13788848]\n800 \ncost: 0.002489171 \nweight: [[0.9421933]] \nbias: [0.13140818]\n820 \ncost: 0.0022607064 \nweight: [[0.94491]] \nbias: [0.12523247]\n840 \ncost: 0.0020532059 \nweight: [[0.94749904]] \nbias: [0.11934702]\n860 \ncost: 0.0018647577 \nweight: [[0.9499664]] \nbias: [0.11373821]\n880 \ncost: 0.0016936068 \nweight: [[0.9523177]] \nbias: [0.10839294]\n900 \ncost: 0.001538162 \nweight: [[0.9545586]] \nbias: [0.10329889]\n920 \ncost: 0.0013969789 \nweight: [[0.9566942]] \nbias: [0.09844425]\n940 \ncost: 0.0012687659 \nweight: [[0.95872945]] \nbias: [0.09381773]\n960 \ncost: 0.0011523066 \nweight: [[0.96066904]] \nbias: [0.08940858]\n980 \ncost: 0.001046548 \nweight: [[0.9625174]] \nbias: [0.0852067]\n1000 \ncost: 0.0009504846 \nweight: [[0.964279]] \nbias: [0.0812023]\n1020 \ncost: 0.0008632462 \nweight: [[0.96595776]] \nbias: [0.07738607]\n1040 \ncost: 0.000784019 \nweight: [[0.9675576]] \nbias: [0.07374922]\n1060 \ncost: 0.0007120559 \nweight: [[0.9690822]] \nbias: [0.0702833]\n1080 \ncost: 0.00064670015 \nweight: [[0.9705352]] \nbias: [0.06698029]\n1100 \ncost: 0.0005873453 \nweight: [[0.97191995]] \nbias: [0.06383247]\n1120 \ncost: 0.00053343776 \nweight: [[0.9732397]] \nbias: [0.06083257]\n1140 \ncost: 0.0004844719 \nweight: [[0.9744973]] \nbias: [0.05797363]\n1160 \ncost: 0.00044000556 \nweight: [[0.97569585]] \nbias: [0.05524907]\n1180 \ncost: 0.00039962307 \nweight: [[0.976838]] \nbias: [0.05265258]\n1200 \ncost: 0.00036294284 \nweight: [[0.9779265]] \nbias: [0.05017814]\n1220 \ncost: 0.00032963083 \nweight: [[0.9789639]] \nbias: [0.04781999]\n1240 \ncost: 0.00029937882 \nweight: [[0.9799525]] \nbias: [0.0455727]\n1260 \ncost: 0.000271901 \nweight: [[0.9808947]] \nbias: [0.04343091]\n1280 \ncost: 0.00024694062 \nweight: [[0.98179257]] \nbias: [0.0413898]\n1300 \ncost: 0.00022427832 \nweight: [[0.98264825]] \nbias: [0.03944463]\n1320 \ncost: 0.000203691 \nweight: [[0.98346364]] \nbias: [0.03759089]\n1340 \ncost: 0.00018499709 \nweight: [[0.98424083]] \nbias: [0.03582429]\n1360 \ncost: 0.00016801793 \nweight: [[0.9849815]] \nbias: [0.03414067]\n1380 \ncost: 0.00015259585 \nweight: [[0.98568726]] \nbias: [0.03253618]\n1400 \ncost: 0.00013859026 \nweight: [[0.9863599]] \nbias: [0.03100708]\n1420 \ncost: 0.0001258694 \nweight: [[0.98700094]] \nbias: [0.02954989]\n1440 \ncost: 0.000114317016 \nweight: [[0.98761183]] \nbias: [0.02816118]\n1460 \ncost: 0.00010382469 \nweight: [[0.988194]] \nbias: [0.02683773]\n1480 \ncost: 9.429627e-05 \nweight: [[0.98874885]] \nbias: [0.02557647]\n1500 \ncost: 8.56414e-05 \nweight: [[0.98927766]] \nbias: [0.02437449]\n1520 \ncost: 7.7781115e-05 \nweight: [[0.9897813]] \nbias: [0.02322906]\n1540 \ncost: 7.064168e-05 \nweight: [[0.9902618]] \nbias: [0.02213746]\n1560 \ncost: 6.41582e-05 \nweight: [[0.99071944]] \nbias: [0.02109702]\n1580 \ncost: 5.8268673e-05 \nweight: [[0.99115556]] \nbias: [0.02010552]\n1600 \ncost: 5.292094e-05 \nweight: [[0.99157125]] \nbias: [0.01916062]\n1620 \ncost: 4.8064063e-05 \nweight: [[0.9919674]] \nbias: [0.01826012]\n1640 \ncost: 4.3651817e-05 \nweight: [[0.99234486]] \nbias: [0.01740194]\n1660 \ncost: 3.9645445e-05 \nweight: [[0.99270463]] \nbias: [0.01658409]\n1680 \ncost: 3.6006928e-05 \nweight: [[0.9930475]] \nbias: [0.0158047]\n1700 \ncost: 3.270112e-05 \nweight: [[0.9933742]] \nbias: [0.01506196]\n1720 \ncost: 2.9700459e-05 \nweight: [[0.9936856]] \nbias: [0.01435411]\n1740 \ncost: 2.6974432e-05 \nweight: [[0.9939823]] \nbias: [0.01367952]\n1760 \ncost: 2.4498595e-05 \nweight: [[0.99426514]] \nbias: [0.01303665]\n1780 \ncost: 2.2250179e-05 \nweight: [[0.9945347]] \nbias: [0.01242398]\n1800 \ncost: 2.0207863e-05 \nweight: [[0.99479157]] \nbias: [0.01184012]\n1820 \ncost: 1.8353476e-05 \nweight: [[0.99503636]] \nbias: [0.01128365]\n1840 \ncost: 1.6668948e-05 \nweight: [[0.9952696]] \nbias: [0.01075334]\n1860 \ncost: 1.5138288e-05 \nweight: [[0.9954919]] \nbias: [0.01024796]\n1880 \ncost: 1.374878e-05 \nweight: [[0.9957038]] \nbias: [0.00976633]\n1900 \ncost: 1.2486987e-05 \nweight: [[0.9959057]] \nbias: [0.00930734]\n1920 \ncost: 1.1340904e-05 \nweight: [[0.9960981]] \nbias: [0.00886992]\n1940 \ncost: 1.0300341e-05 \nweight: [[0.99628145]] \nbias: [0.00845306]\n1960 \ncost: 9.354503e-06 \nweight: [[0.99645627]] \nbias: [0.0080558]\n1980 \ncost: 8.496329e-06 \nweight: [[0.9966228]] \nbias: [0.0076772]\n2000 \ncost: 7.716275e-06 \nweight: [[0.9967815]] \nbias: [0.00731637]\n[[4.9912243]]\n[[2.4992702]]\n[[1.5024886]\n [3.4960518]]\n"
    }
   ],
   "source": [
    "# Lab 2 Linear Regression \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "torch.manual_seed(111) # for reproductibility\n",
    "\n",
    "# X and Y data\n",
    "x_train=[[1],[2],[3]]\n",
    "y_train=[[1],[2],[3]]\n",
    "# Change list to torch variable\n",
    "X=Variable(torch.Tensor(x_train))\n",
    "Y=Variable(torch.Tensor(y_train))\n",
    "\n",
    "# Out hypothesis WX+B (linear)\n",
    "model=nn.Linear(1,1,bias=True) #Set layer when ser model. It's different with TF2\n",
    "\n",
    "# cost function\n",
    "criterion=nn.MSELoss() # Torch say loss or cost criterion\n",
    "\n",
    "# Minimize\n",
    "optimizer=torch.optim.SGD(model.parameters(),lr=0.01) #paramaters()를 호출하면 모델의 맴버인 nn.Linear 모듈의 학습 가능한 매개변수들이 포하된다.\n",
    "\n",
    "for step in range(2001):\n",
    "    hypothesis=model(X)\n",
    "    cost=criterion(hypothesis,Y)\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if step%20==0:\n",
    "        print(step,'\\ncost:',cost.data.numpy(),'\\nweight:',model.weight.data.numpy(),'\\nbias:',model.bias.data.numpy())\n",
    "#Testing out model\n",
    "predicted=model(Variable(torch.Tensor([[5]])))\n",
    "print(predicted.data.numpy())\n",
    "predicted=model(Variable(torch.Tensor([[2.5]])))\n",
    "print(predicted.data.numpy())\n",
    "predicted=model(Variable(torch.Tensor([[1.5],[3.5]])))\n",
    "print(predicted.data.numpy())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 한번 더 연습\n",
    "    - y_data=2*x_data 의 관계인데, 훈련 결과를  predict했을 때 Y=4*X 관계의 결과가 나옴.\n",
    "    - TF2의 경우 더 정확하게 예측 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0 72.668076 [[0.44615632]] [0.32038504]\n20 6.6155868 [[1.473104]] [0.47777417]\n40 0.6723169 [[1.781789]] [0.52069014]\n60 0.13598771 [[1.8750452]] [0.52932185]\n80 0.08605681 [[1.9036832]] [0.5277186]\n100 0.07991602 [[1.9129331]] [0.5230927]\n120 0.077751145 [[1.9163611]] [0.5176069]\n140 0.07597983 [[1.9180351]] [0.5119091]\n160 0.0742792 [[1.9191765]] [0.5061933]\n180 0.07261945 [[1.9201506]] [0.50051725]\n200 0.070996925 [[1.9210675]] [0.49489748]\n220 0.06941067 [[1.9219601]] [0.4893387]\n240 0.0678599 [[1.9228387]] [0.48384166]\n260 0.06634381 [[1.923706]] [0.4784062]\n280 0.064861454 [[1.9245633]] [0.4730317]\n300 0.0634124 [[1.9254109]] [0.46771765]\n320 0.061995566 [[1.9262489]] [0.4624632]\n340 0.060610563 [[1.9270774]] [0.45726782]\n360 0.059256304 [[1.9278966]] [0.45213082]\n380 0.057932455 [[1.9287065]] [0.4470515]\n400 0.05663802 [[1.9295075]] [0.44202927]\n420 0.055372607 [[1.9302994]] [0.43706346]\n440 0.0541355 [[1.9310825]] [0.4321534]\n460 0.052926023 [[1.9318566]] [0.42729855]\n480 0.051743545 [[1.9326222]] [0.42249817]\n500 0.050587527 [[1.933379]] [0.41775185]\n520 0.04945727 [[1.9341274]] [0.41305876]\n540 0.04835231 [[1.9348675]] [0.4084184]\n560 0.04727202 [[1.9355992]] [0.4038301]\n580 0.046215873 [[1.9363228]] [0.3992934]\n600 0.045183245 [[1.9370381]] [0.3948077]\n620 0.044173777 [[1.9377453]] [0.3903724]\n640 0.04318684 [[1.9384447]] [0.38598692]\n660 0.042221982 [[1.9391364]] [0.38165072]\n680 0.04127866 [[1.93982]] [0.37736318]\n700 0.040356386 [[1.9404961]] [0.3731239]\n720 0.039454833 [[1.9411645]] [0.3689322]\n740 0.038573254 [[1.9418256]] [0.36478758]\n760 0.03771149 [[1.9424791]] [0.3606895]\n780 0.03686893 [[1.9431254]] [0.35663745]\n800 0.036045164 [[1.9437643]] [0.35263094]\n820 0.035239838 [[1.944396]] [0.34866947]\n840 0.034452524 [[1.9450207]] [0.3447525]\n860 0.033682834 [[1.9456383]] [0.34087947]\n880 0.032930277 [[1.9462491]] [0.33705002]\n900 0.032194544 [[1.9468528]] [0.33326355]\n920 0.03147521 [[1.9474499]] [0.32951963]\n940 0.030772042 [[1.9480402]] [0.32581782]\n960 0.030084532 [[1.948624]] [0.32215756]\n980 0.029412383 [[1.9492011]] [0.31853837]\n1000 0.028755197 [[1.9497718]] [0.31495985]\n1020 0.028112775 [[1.9503361]] [0.31142154]\n1040 0.02748469 [[1.9508941]] [0.30792302]\n1060 0.026870597 [[1.9514457]] [0.30446374]\n1080 0.026270246 [[1.9519912]] [0.30104333]\n1100 0.025683368 [[1.9525305]] [0.29766133]\n1120 0.025109533 [[1.9530637]] [0.29431742]\n1140 0.024548525 [[1.953591]] [0.291011]\n1160 0.024000052 [[1.9541124]] [0.28774175]\n1180 0.023463871 [[1.9546279]] [0.28450927]\n1200 0.022939611 [[1.9551376]] [0.28131297]\n1220 0.02242707 [[1.9556416]] [0.27815244]\n1240 0.021925995 [[1.95614]] [0.27502766]\n1260 0.021436121 [[1.9566329]] [0.2719379]\n1280 0.020957211 [[1.95712]] [0.26888293]\n1300 0.020488992 [[1.9576017]] [0.26586226]\n1320 0.02003121 [[1.958078]] [0.26287553]\n1340 0.01958366 [[1.9585489]] [0.25992236]\n1360 0.01914614 [[1.9590145]] [0.25700238]\n1380 0.018718328 [[1.9594749]] [0.25411522]\n1400 0.018300166 [[1.9599302]] [0.25126043]\n1420 0.017891277 [[1.9603803]] [0.24843776]\n1440 0.017491579 [[1.9608256]] [0.24564679]\n1460 0.01710077 [[1.9612657]] [0.24288715]\n1480 0.016718734 [[1.9617008]] [0.24015853]\n1500 0.016345188 [[1.962131]] [0.23746055]\n1520 0.015979987 [[1.9625565]] [0.23479292]\n1540 0.015622979 [[1.962977]] [0.23215526]\n1560 0.015273912 [[1.963393]] [0.22954723]\n1580 0.014932674 [[1.9638042]] [0.2269685]\n1600 0.014599057 [[1.9642109]] [0.22441871]\n1620 0.014272903 [[1.964613]] [0.22189757]\n1640 0.013954006 [[1.9650104]] [0.21940476]\n1660 0.013642235 [[1.9654034]] [0.21693994]\n1680 0.013337426 [[1.9657921]] [0.21450283]\n1700 0.013039449 [[1.9661765]] [0.21209307]\n1720 0.012748113 [[1.9665564]] [0.20971037]\n1740 0.012463298 [[1.966932]] [0.20735447]\n1760 0.012184859 [[1.9673036]] [0.20502503]\n1780 0.01191262 [[1.9676709]] [0.20272174]\n1800 0.011646474 [[1.9680343]] [0.20044434]\n1820 0.01138626 [[1.9683932]] [0.19819252]\n1840 0.011131876 [[1.9687482]] [0.195966]\n1860 0.010883141 [[1.9690994]] [0.19376451]\n1880 0.010639998 [[1.9694464]] [0.19158775]\n1900 0.010402264 [[1.9697897]] [0.18943541]\n1920 0.010169895 [[1.9701293]] [0.18730722]\n1940 0.009942638 [[1.9704646]] [0.18520297]\n1960 0.0097205285 [[1.9707966]] [0.1831224]\n1980 0.009503362 [[1.9711245]] [0.1810652]\n2000 0.009291019 [[1.971449]] [0.1790311]\n[[19.96504 ]\n [23.85165 ]\n [31.624872]\n [12.191815]]\n"
    }
   ],
   "source": [
    "# Lab 2 Linear Regression\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "torch.manual_seed(999)\n",
    "\n",
    "x_data=[[i] for i in range(10)]\n",
    "y_data=[[i] for i in range(0,20,2)]\n",
    "# Change list to torch.Tensor\n",
    "X=Variable(torch.Tensor(x_data))\n",
    "Y=Variable(torch.Tensor(y_data))\n",
    "\n",
    "# Set model\n",
    "# Our Model in Linear\n",
    "model=nn.Linear(1,1,bias=True)\n",
    "\n",
    "# Set Criterion\n",
    "criterion=nn.MSELoss()\n",
    "\n",
    "# Minimize\n",
    "optimizer=torch.optim.SGD(model.parameters(),lr=0.001) #model의 parameters를 (매개변수를) 포함시키겠다.\n",
    "#paramaters()를 호출하면 모델의 맴버인 nn.Linear 모듈의 학습 가능한 매개변수들이 포하된다.\n",
    "\n",
    "#Train the midel\n",
    "for step in range(2001):\n",
    "    hypothesis=model(X) #1 hypothesis \n",
    "    cost=criterion(hypothesis,Y) #2 cost function\n",
    "    optimizer.zero_grad() #3 기울기 값 0 셋팅\n",
    "    cost.backward() #편미분\n",
    "    optimizer.step() #5 w,b update\n",
    "\n",
    "    if step%20==0:\n",
    "        print(step,cost.data.numpy(),model.weight.data.numpy(),model.bias.data.numpy())\n",
    "    \n",
    "# Testing our model by using other data\n",
    "predicted=model(model(Variable(torch.Tensor([[5],[6],[8],[3]]))))\n",
    "print(predicted.data.numpy())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9]]\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[[0], [2], [4], [6], [8], [10], [12], [14], [16], [18]]"
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    "x_data=[[i] for i in range(10)]\n",
    "print(x_data)\n",
    "y_data=[[i] for i in range(0,20,2)]\n",
    "y_data"
   ]
  }
 ]
}