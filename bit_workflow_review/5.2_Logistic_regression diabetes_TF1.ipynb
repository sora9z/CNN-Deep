{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1601608986854",
   "display_name": "Python 3.6.10 64-bit ('TF1': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# 5.2 Logistic Regression Diabletes -- TF1\n",
    "\n",
    "## (1) 실습 목적\n",
    "    - Logistic regression을 파일로 읽는다.\n",
    "## (2) 사용한 API 및 간단한 설명\n",
    "    - xy=np.loadtxt('path or file name on current directory,dilimiter=',',dtype=np.float32)\n",
    "            :file로부터 데이터를 읽음\n",
    "    - predicted=tf.cast(hypothesis>0.5,dtype=tf.float32) hypothesis가 0.5 이상이면 1 미만이면 0 반환\n",
    "    - accuracy=tf.reduce_mean(tf.cast(tf.eqau(predicted,Y),dtype=tf.float32)) predicted와 Y가 같으면 1 아니면 0 반환 ==> 에 대한 합의 평균\n",
    "    \n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[-0.294118    0.487437    0.180328   -0.292929    0.          0.00149028\n  -0.53117    -0.0333333   0.        ]]\n0 0.6056365\n200 0.59589493\n400 0.58711785\n600 0.57917655\n800 0.5719758\n1000 0.5654338\n1200 0.5594783\n1400 0.554046\n1600 0.5490804\n1800 0.54453236\n2000 0.54035825\n2200 0.53651935\n2400 0.53298205\n2600 0.52971625\n2800 0.52669555\n3000 0.5238964\n3200 0.52129805\n3400 0.51888216\n3600 0.51663214\n3800 0.5145332\n4000 0.5125727\n4200 0.5107385\n4400 0.50902003\n4600 0.5074082\n4800 0.50589406\n5000 0.5044704\n5200 0.5031296\n5400 0.501866\n5600 0.50067365\n5800 0.49954715\n6000 0.4984821\n6200 0.49747407\n6400 0.49651894\n6600 0.4956133\n6800 0.49475375\n7000 0.49393725\n7200 0.49316114\n7400 0.49242258\n7600 0.49171948\n7800 0.49104956\n8000 0.4904107\n8200 0.4898011\n8400 0.489219\n8600 0.488663\n8800 0.48813128\n9000 0.4876225\n9200 0.4871357\n9400 0.48666942\n9600 0.48622254\n9800 0.4857938\n10000 0.48538265\n\n Hypothesis :  [[0.39802504]\n [0.91985637]\n [0.17529216]\n [0.9524356 ]\n [0.14275852]\n [0.73331434]\n [0.9498289 ]\n [0.6172892 ]\n [0.24977767]\n [0.520999  ]\n [0.70208895]\n [0.16600472]\n [0.18126646]\n [0.2650246 ]\n [0.72349703]\n [0.4851254 ]\n [0.71410805]\n [0.90759504]\n [0.8279103 ]\n [0.57306933]\n [0.6680923 ]\n [0.10142091]\n [0.6291302 ]\n [0.7036674 ]\n [0.3854922 ]\n [0.92909646]\n [0.5496567 ]\n [0.62211657]\n [0.7160565 ]\n [0.39378956]\n [0.9546783 ]\n [0.83274645]\n [0.5496368 ]\n [0.7906609 ]\n [0.3463115 ]\n [0.62455475]\n [0.8263968 ]\n [0.51338756]\n [0.48970187]\n [0.34926158]\n [0.80725694]\n [0.15898475]\n [0.38669035]\n [0.04117003]\n [0.55223274]\n [0.9199775 ]\n [0.7304633 ]\n [0.71406394]\n [0.9248066 ]\n [0.9341142 ]\n [0.93279135]\n [0.23100832]\n [0.34380376]\n [0.9669477 ]\n [0.23048097]\n [0.483861  ]\n [0.09320819]\n [0.7599648 ]\n [0.8841336 ]\n [0.50113666]\n [0.9403069 ]\n [0.68392086]\n [0.669678  ]\n [0.8470376 ]\n [0.54027474]\n [0.57144916]\n [0.954244  ]\n [0.69205034]\n [0.8518604 ]\n [0.66499406]\n [0.26544824]\n [0.72401106]\n [0.91201967]\n [0.9276566 ]\n [0.87555057]\n [0.79652345]\n [0.45808113]\n [0.85182905]\n [0.89584494]\n [0.92429495]\n [0.8567951 ]\n [0.8064703 ]\n [0.39235154]\n [0.8068647 ]\n [0.5566476 ]\n [0.89350736]\n [0.4735089 ]\n [0.8936173 ]\n [0.93668884]\n [0.7733934 ]\n [0.858418  ]\n [0.6474477 ]\n [0.69622767]\n [0.6055814 ]\n [0.90824723]\n [0.97661746]\n [0.9015565 ]\n [0.6514673 ]\n [0.21107978]\n [0.62874675]\n [0.56733143]\n [0.96083015]\n [0.757236  ]\n [0.7658889 ]\n [0.84178215]\n [0.71158665]\n [0.9348029 ]\n [0.8381529 ]\n [0.513725  ]\n [0.34828216]\n [0.93784773]\n [0.8727175 ]\n [0.43643543]\n [0.41439375]\n [0.637866  ]\n [0.8329273 ]\n [0.8562988 ]\n [0.9283109 ]\n [0.16648746]\n [0.729746  ]\n [0.85951245]\n [0.63067544]\n [0.61242807]\n [0.8819038 ]\n [0.74241084]\n [0.8591677 ]\n [0.8230977 ]\n [0.57471   ]\n [0.513325  ]\n [0.36144188]\n [0.4473757 ]\n [0.78508914]\n [0.9250659 ]\n [0.8619108 ]\n [0.80292165]\n [0.8499434 ]\n [0.3963813 ]\n [0.817857  ]\n [0.7112351 ]\n [0.74478483]\n [0.8975791 ]\n [0.63991517]\n [0.5809531 ]\n [0.6987425 ]\n [0.91332114]\n [0.701947  ]\n [0.47958606]\n [0.9299488 ]\n [0.6462492 ]\n [0.74826616]\n [0.22493169]\n [0.36101794]\n [0.13890061]\n [0.27311116]\n [0.9181652 ]\n [0.8606795 ]\n [0.94551325]\n [0.11876246]\n [0.488706  ]\n [0.81195974]\n [0.6531626 ]\n [0.8740402 ]\n [0.3569466 ]\n [0.7960932 ]\n [0.612386  ]\n [0.60120815]\n [0.71109706]\n [0.8557689 ]\n [0.75913346]\n [0.63471407]\n [0.86924124]\n [0.8987445 ]\n [0.9558128 ]\n [0.21311182]\n [0.8088621 ]\n [0.32837754]\n [0.44200772]\n [0.36063778]\n [0.866822  ]\n [0.669081  ]\n [0.93315566]\n [0.90341926]\n [0.5784775 ]\n [0.13214087]\n [0.18069613]\n [0.52752936]\n [0.7118552 ]\n [0.6259881 ]\n [0.83207285]\n [0.6401884 ]\n [0.34331647]\n [0.2562955 ]\n [0.9043266 ]\n [0.3962045 ]\n [0.87101424]\n [0.8879822 ]\n [0.7088567 ]\n [0.66104674]\n [0.5809884 ]\n [0.58096945]\n [0.66213775]\n [0.94447446]\n [0.805295  ]\n [0.7820085 ]\n [0.14068592]\n [0.32125598]\n [0.9251598 ]\n [0.20322174]\n [0.93259156]\n [0.27457976]\n [0.26301652]\n [0.5174599 ]\n [0.7200251 ]\n [0.21616611]\n [0.7682073 ]\n [0.7231012 ]\n [0.7489024 ]\n [0.6973084 ]\n [0.13305768]\n [0.3186885 ]\n [0.6979062 ]\n [0.53338826]\n [0.9166151 ]\n [0.94621426]\n [0.69563293]\n [0.3684857 ]\n [0.01899466]\n [0.7178898 ]\n [0.34307414]\n [0.5150941 ]\n [0.9434208 ]\n [0.6342741 ]\n [0.9527333 ]\n [0.23619458]\n [0.12806383]\n [0.22542807]\n [0.6812804 ]\n [0.92249775]\n [0.8862499 ]\n [0.6204542 ]\n [0.6161144 ]\n [0.61719495]\n [0.10930198]\n [0.55129105]\n [0.13586503]\n [0.5853985 ]\n [0.85928607]\n [0.68006104]\n [0.6709457 ]\n [0.9482722 ]\n [0.8179749 ]\n [0.74352103]\n [0.7730067 ]\n [0.7583226 ]\n [0.8599657 ]\n [0.36005062]\n [0.43262696]\n [0.4741838 ]\n [0.797449  ]\n [0.6800664 ]\n [0.69556   ]\n [0.805148  ]\n [0.28877744]\n [0.45827615]\n [0.5383282 ]\n [0.6150569 ]\n [0.38749364]\n [0.91906613]\n [0.7360227 ]\n [0.9380857 ]\n [0.5723773 ]\n [0.79293764]\n [0.80593   ]\n [0.81664646]\n [0.63072884]\n [0.83450997]\n [0.35456032]\n [0.61541337]\n [0.6892328 ]\n [0.35010898]\n [0.8043666 ]\n [0.26666114]\n [0.6198478 ]\n [0.936296  ]\n [0.81097376]\n [0.8681171 ]\n [0.70008075]\n [0.47212318]\n [0.6915759 ]\n [0.41178247]\n [0.46886215]\n [0.62662846]\n [0.5915576 ]\n [0.66667825]\n [0.5601518 ]\n [0.17932093]\n [0.6972097 ]\n [0.92046344]\n [0.5433219 ]\n [0.5849363 ]\n [0.799564  ]\n [0.47336662]\n [0.7384504 ]\n [0.4250204 ]\n [0.6889992 ]\n [0.8941389 ]\n [0.67197514]\n [0.68447983]\n [0.8678657 ]\n [0.49784255]\n [0.87166417]\n [0.93566906]\n [0.28175855]\n [0.82353413]\n [0.2360731 ]\n [0.76892567]\n [0.80791384]\n [0.6835873 ]\n [0.3123654 ]\n [0.8141378 ]\n [0.73861504]\n [0.7558252 ]\n [0.1879037 ]\n [0.85274965]\n [0.8696192 ]\n [0.45188066]\n [0.95375794]\n [0.3065834 ]\n [0.66513807]\n [0.9508338 ]\n [0.2583835 ]\n [0.44389454]\n [0.67912847]\n [0.29849553]\n [0.18244267]\n [0.8357005 ]\n [0.9105977 ]\n [0.85220015]\n [0.6067387 ]\n [0.6847021 ]\n [0.60895574]\n [0.7760563 ]\n [0.7973491 ]\n [0.93321097]\n [0.7581532 ]\n [0.7855036 ]\n [0.56120926]\n [0.9399078 ]\n [0.9409902 ]\n [0.7570299 ]\n [0.25185883]\n [0.6874559 ]\n [0.3742432 ]\n [0.7848005 ]\n [0.18836999]\n [0.23501766]\n [0.4437033 ]\n [0.705515  ]\n [0.40359205]\n [0.5743315 ]\n [0.86300826]\n [0.61635935]\n [0.8432493 ]\n [0.944767  ]\n [0.76018053]\n [0.0534994 ]\n [0.41632813]\n [0.86655974]\n [0.8834223 ]\n [0.69287705]\n [0.2958198 ]\n [0.86977124]\n [0.9107295 ]\n [0.3485073 ]\n [0.597414  ]\n [0.8285879 ]\n [0.8153161 ]\n [0.85892   ]\n [0.87991214]\n [0.87246126]\n [0.9244726 ]\n [0.65475684]\n [0.6331799 ]\n [0.56147045]\n [0.82503045]\n [0.8888284 ]\n [0.25635722]\n [0.8042054 ]\n [0.8657925 ]\n [0.28334948]\n [0.52692026]\n [0.8522356 ]\n [0.5531823 ]\n [0.89192957]\n [0.27486944]\n [0.8415382 ]\n [0.6421059 ]\n [0.874069  ]\n [0.3813337 ]\n [0.7544021 ]\n [0.71157575]\n [0.7551615 ]\n [0.06720123]\n [0.21959886]\n [0.6399102 ]\n [0.82339585]\n [0.42882478]\n [0.80390227]\n [0.5277596 ]\n [0.34463045]\n [0.79892707]\n [0.4514246 ]\n [0.89278114]\n [0.82321537]\n [0.6969688 ]\n [0.9215793 ]\n [0.7227062 ]\n [0.80877876]\n [0.36355513]\n [0.29516912]\n [0.7561327 ]\n [0.46806628]\n [0.47671396]\n [0.910632  ]\n [0.88759506]\n [0.9142057 ]\n [0.9496678 ]\n [0.66453314]\n [0.8564842 ]\n [0.41091907]\n [0.36583883]\n [0.44283423]\n [0.9369325 ]\n [0.5566578 ]\n [0.14684078]\n [0.9336991 ]\n [0.8344946 ]\n [0.5138866 ]\n [0.80019015]\n [0.01735631]\n [0.9123043 ]\n [0.78261626]\n [0.7628877 ]\n [0.7706383 ]\n [0.96169907]\n [0.5938005 ]\n [0.79826105]\n [0.69892675]\n [0.8790553 ]\n [0.19820574]\n [0.6252237 ]\n [0.91263616]\n [0.6336706 ]\n [0.7144262 ]\n [0.9332372 ]\n [0.85426223]\n [0.8735813 ]\n [0.3849861 ]\n [0.7879621 ]\n [0.9481963 ]\n [0.7621105 ]\n [0.6504078 ]\n [0.34851265]\n [0.46533918]\n [0.52343184]\n [0.63433594]\n [0.5066643 ]\n [0.7751276 ]\n [0.5617325 ]\n [0.76567924]\n [0.80948657]\n [0.7434161 ]\n [0.63490725]\n [0.51006764]\n [0.57288224]\n [0.94176483]\n [0.85594743]\n [0.2755723 ]\n [0.47524855]\n [0.5461467 ]\n [0.11392623]\n [0.874698  ]\n [0.13108853]\n [0.91008735]\n [0.87351954]\n [0.8427391 ]\n [0.6499781 ]\n [0.8963007 ]\n [0.35333717]\n [0.75432277]\n [0.9388436 ]\n [0.32096428]\n [0.40109852]\n [0.87178844]\n [0.88187635]\n [0.66034234]\n [0.8214715 ]\n [0.82771325]\n [0.7974217 ]\n [0.28421354]\n [0.77149653]\n [0.9072437 ]\n [0.5979472 ]\n [0.7862836 ]\n [0.7126355 ]\n [0.80740535]\n [0.84776556]\n [0.9382832 ]\n [0.6244032 ]\n [0.3818223 ]\n [0.7889751 ]\n [0.7074686 ]\n [0.96791553]\n [0.772712  ]\n [0.7076293 ]\n [0.39529753]\n [0.7267734 ]\n [0.9181938 ]\n [0.95060074]\n [0.89538807]\n [0.68919516]\n [0.602396  ]\n [0.80347884]\n [0.47540376]\n [0.83051187]\n [0.7861556 ]\n [0.88750005]\n [0.6141226 ]\n [0.6844928 ]\n [0.88197494]\n [0.4582334 ]\n [0.4918039 ]\n [0.6774914 ]\n [0.72436595]\n [0.63142145]\n [0.91739166]\n [0.92741835]\n [0.19802749]\n [0.14832553]\n [0.78469396]\n [0.5526471 ]\n [0.17326167]\n [0.84543866]\n [0.90453917]\n [0.6883518 ]\n [0.94070756]\n [0.92874956]\n [0.7597133 ]\n [0.85565364]\n [0.69081277]\n [0.6332032 ]\n [0.7476649 ]\n [0.63379085]\n [0.13207912]\n [0.9172018 ]\n [0.8839382 ]\n [0.67560446]\n [0.9132729 ]\n [0.8957464 ]\n [0.9066962 ]\n [0.60275406]\n [0.72259307]\n [0.8893317 ]\n [0.68211114]\n [0.86432517]\n [0.925166  ]\n [0.5191848 ]\n [0.8465353 ]\n [0.8219192 ]\n [0.569306  ]\n [0.5015137 ]\n [0.09883937]\n [0.2750439 ]\n [0.82167906]\n [0.6292344 ]\n [0.70153004]\n [0.5055436 ]\n [0.9278692 ]\n [0.46752098]\n [0.7897107 ]\n [0.24467653]\n [0.8744857 ]\n [0.33983552]\n [0.805613  ]\n [0.5728727 ]\n [0.8224063 ]\n [0.56832963]\n [0.20769912]\n [0.8289814 ]\n [0.95496154]\n [0.41190243]\n [0.92591846]\n [0.8430282 ]\n [0.83709013]\n [0.80448765]\n [0.43344963]\n [0.3179881 ]\n [0.7234218 ]\n [0.1564179 ]\n [0.94451225]\n [0.3650126 ]\n [0.9263969 ]\n [0.8937538 ]\n [0.43915528]\n [0.18817848]\n [0.6399529 ]\n [0.46073866]\n [0.8167179 ]\n [0.66470057]\n [0.97878206]\n [0.45462617]\n [0.6392232 ]\n [0.80139303]\n [0.692894  ]\n [0.05536649]\n [0.8074494 ]\n [0.8250225 ]\n [0.85398793]\n [0.62090826]\n [0.45765746]\n [0.61745787]\n [0.90644896]\n [0.58525443]\n [0.7878934 ]\n [0.80004656]\n [0.8554873 ]\n [0.78917   ]\n [0.54141164]\n [0.7927105 ]\n [0.89632577]\n [0.7116288 ]\n [0.95429355]\n [0.7659631 ]\n [0.6163018 ]\n [0.48374906]\n [0.81102735]\n [0.82940125]\n [0.512873  ]\n [0.63237566]\n [0.26901466]\n [0.5541273 ]\n [0.8026892 ]\n [0.9513036 ]\n [0.8525419 ]\n [0.74860805]\n [0.7404382 ]\n [0.9015614 ]\n [0.5400938 ]\n [0.9310826 ]\n [0.55276644]\n [0.8156839 ]\n [0.2945621 ]\n [0.0709911 ]\n [0.28177238]\n [0.3557998 ]\n [0.72199863]\n [0.838966  ]\n [0.6180975 ]\n [0.73455   ]\n [0.835273  ]\n [0.5242059 ]\n [0.3941626 ]\n [0.8999509 ]\n [0.8945167 ]\n [0.4323228 ]\n [0.6776854 ]\n [0.17364365]\n [0.35423085]\n [0.7534952 ]\n [0.7420581 ]\n [0.889035  ]\n [0.97808504]\n [0.22462064]\n [0.77354574]\n [0.57108265]\n [0.4400772 ]\n [0.7156601 ]\n [0.69504136]\n [0.9012685 ]\n [0.67071027]\n [0.56781316]\n [0.5639212 ]\n [0.1497562 ]\n [0.68539566]\n [0.5781162 ]\n [0.9012623 ]\n [0.5486102 ]\n [0.57768595]\n [0.7685056 ]\n [0.71108264]\n [0.4679773 ]\n [0.7591648 ]\n [0.61242545]\n [0.29799378]\n [0.6514271 ]\n [0.8891001 ]\n [0.8510603 ]\n [0.5956233 ]\n [0.81887984]\n [0.29271138]\n [0.8568601 ]\n [0.59574217]\n [0.7613163 ]\n [0.44835398]\n [0.66375065]\n [0.8316531 ]\n [0.1657677 ]\n [0.26608562]\n [0.77673984]\n [0.8349063 ]\n [0.8060243 ]\n [0.8958596 ]\n [0.83290607]\n [0.7165599 ]\n [0.7644323 ]\n [0.7620305 ]\n [0.72156006]\n [0.79902995]\n [0.44361314]\n [0.3665706 ]\n [0.89137614]\n [0.79791915]\n [0.5915617 ]\n [0.3017752 ]\n [0.88355005]\n [0.7875365 ]\n [0.8479537 ]\n [0.6485733 ]\n [0.8768362 ]\n [0.89538264]\n [0.7985251 ]\n [0.43921953]\n [0.9092064 ]\n [0.9197307 ]\n [0.27993244]\n [0.14103171]\n [0.71454555]\n [0.43450645]\n [0.8200825 ]\n [0.3535746 ]\n [0.44442138]\n [0.3518371 ]\n [0.80504036]\n [0.86214036]\n [0.14409044]\n [0.3616558 ]\n [0.5800122 ]\n [0.46463194]\n [0.54496455]\n [0.79959494]\n [0.16587731]\n [0.9149399 ]\n [0.21074921]\n [0.83420944]\n [0.7568208 ]\n [0.7399765 ]\n [0.8275947 ]\n [0.7273352 ]\n [0.89022744]] \n Correct (Y) : [[0.]\n [1.]\n [0.]\n [1.]\n [0.]\n [1.]\n [1.]\n [1.]\n [0.]\n [1.]\n [1.]\n [0.]\n [0.]\n [0.]\n [1.]\n [0.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [0.]\n [1.]\n [1.]\n [0.]\n [1.]\n [1.]\n [1.]\n [1.]\n [0.]\n [1.]\n [1.]\n [1.]\n [1.]\n [0.]\n [1.]\n [1.]\n [1.]\n [0.]\n [0.]\n [1.]\n [0.]\n [0.]\n [0.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [0.]\n [0.]\n [1.]\n [0.]\n [0.]\n [0.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [0.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [0.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [0.]\n [1.]\n [1.]\n [1.]\n [0.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [0.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [0.]\n [1.]\n [1.]\n [0.]\n [0.]\n [1.]\n [1.]\n [1.]\n [1.]\n [0.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [0.]\n [0.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [0.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [0.]\n [1.]\n [1.]\n [1.]\n [0.]\n [0.]\n [0.]\n [0.]\n [1.]\n [1.]\n [1.]\n [0.]\n [0.]\n [1.]\n [1.]\n [1.]\n [0.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [0.]\n [1.]\n [0.]\n [0.]\n [0.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [0.]\n [0.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [0.]\n [0.]\n [1.]\n [0.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [0.]\n [0.]\n [1.]\n [0.]\n [1.]\n [0.]\n [0.]\n [1.]\n [1.]\n [0.]\n [1.]\n [1.]\n [1.]\n [1.]\n [0.]\n [0.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [0.]\n [0.]\n [1.]\n [0.]\n [1.]\n [1.]\n [1.]\n [1.]\n [0.]\n [0.]\n [0.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [0.]\n [1.]\n [0.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [0.]\n [0.]\n [0.]\n [1.]\n [1.]\n [1.]\n [1.]\n [0.]\n [0.]\n [1.]\n [1.]\n [0.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [0.]\n [1.]\n [1.]\n [0.]\n [1.]\n [0.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [0.]\n [1.]\n [0.]\n [0.]\n [1.]\n [1.]\n [1.]\n [1.]\n [0.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [0.]\n [1.]\n [0.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [0.]\n [1.]\n [1.]\n [0.]\n [1.]\n [0.]\n [1.]\n [1.]\n [1.]\n [0.]\n [1.]\n [1.]\n [1.]\n [0.]\n [1.]\n [1.]\n [0.]\n [1.]\n [0.]\n [1.]\n [1.]\n [0.]\n [0.]\n [1.]\n [0.]\n [0.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [0.]\n [1.]\n [0.]\n [1.]\n [0.]\n [0.]\n [0.]\n [1.]\n [0.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [0.]\n [0.]\n [1.]\n [1.]\n [1.]\n [0.]\n [1.]\n [1.]\n [0.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [0.]\n [1.]\n [1.]\n [0.]\n [1.]\n [1.]\n [1.]\n [1.]\n [0.]\n [1.]\n [1.]\n [1.]\n [0.]\n [1.]\n [1.]\n [1.]\n [0.]\n [0.]\n [1.]\n [1.]\n [0.]\n [1.]\n [1.]\n [0.]\n [1.]\n [0.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [0.]\n [0.]\n [1.]\n [0.]\n [0.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [0.]\n [0.]\n [0.]\n [1.]\n [1.]\n [0.]\n [1.]\n [1.]\n [1.]\n [1.]\n [0.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [0.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [0.]\n [1.]\n [1.]\n [1.]\n [1.]\n [0.]\n [0.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [0.]\n [0.]\n [1.]\n [0.]\n [1.]\n [0.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [0.]\n [1.]\n [1.]\n [0.]\n [0.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [0.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [0.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [0.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [0.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [0.]\n [0.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [0.]\n [0.]\n [1.]\n [1.]\n [0.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [0.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [0.]\n [0.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [0.]\n [1.]\n [0.]\n [1.]\n [0.]\n [1.]\n [1.]\n [1.]\n [1.]\n [0.]\n [1.]\n [1.]\n [0.]\n [1.]\n [1.]\n [1.]\n [1.]\n [0.]\n [0.]\n [1.]\n [0.]\n [1.]\n [0.]\n [1.]\n [1.]\n [0.]\n [0.]\n [1.]\n [0.]\n [1.]\n [1.]\n [1.]\n [0.]\n [1.]\n [1.]\n [1.]\n [0.]\n [1.]\n [1.]\n [1.]\n [1.]\n [0.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [0.]\n [1.]\n [1.]\n [1.]\n [1.]\n [0.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [0.]\n [0.]\n [0.]\n [0.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [0.]\n [1.]\n [1.]\n [0.]\n [1.]\n [0.]\n [0.]\n [1.]\n [1.]\n [1.]\n [1.]\n [0.]\n [1.]\n [1.]\n [0.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [0.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [0.]\n [1.]\n [1.]\n [0.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [0.]\n [1.]\n [1.]\n [1.]\n [0.]\n [1.]\n [1.]\n [0.]\n [0.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [0.]\n [0.]\n [1.]\n [1.]\n [1.]\n [0.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [0.]\n [1.]\n [1.]\n [0.]\n [0.]\n [1.]\n [0.]\n [1.]\n [0.]\n [0.]\n [0.]\n [1.]\n [1.]\n [0.]\n [0.]\n [1.]\n [0.]\n [1.]\n [1.]\n [0.]\n [1.]\n [0.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]] \n Accuracy : 0.7628459\n"
    }
   ],
   "source": [
    "# Lab 5-2 Logistic Regression Classifier\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "tf.set_random_seed(000)\n",
    "\n",
    "#1. row data import & split the data set into x_data,y_data\n",
    "xy=np.loadtxt(\"00_data/data-03-diabetes.csv\",delimiter=',',dtype=np.float32)\n",
    "\n",
    "#Check the loaded data is OK\n",
    "if xy is None:\n",
    "    print(\"data load failed\")\n",
    "print(xy[[0],:])\n",
    "\n",
    "\n",
    "x_data=xy[:,:-1]\n",
    "y_data=xy[:,[-1]]\n",
    "#print(x_data,y_data)\n",
    "#2. Set weight and bias tf variables & Place holders for input data\n",
    "W=tf.Variable(tf.random_normal([8,1]),name='Weight')\n",
    "b=tf.Variable(tf.random_normal([1]),name='bias')\n",
    "X=tf.placeholder(tf.float32,shape=[None,8])\n",
    "Y=tf.placeholder(tf.float32,shape=[None,1])\n",
    "\n",
    "#3. Set hypothesis \n",
    "hypothesis=tf.sigmoid(tf.matmul(X,W)+b)\n",
    "#4. cost function & Optimizer\n",
    "cost=-tf.reduce_mean(Y*tf.log(hypothesis)+(1-Y)*tf.log(1-hypothesis))\n",
    "train=tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
    "\n",
    "#5.Accuracy computation\n",
    "# True if hypothesis>0.5 else False\n",
    "predicted=tf.cast(hypothesis>0.5,dtype=tf.float32)\n",
    "accuracy=tf.reduce_mean(tf.cast(tf.equal(predicted,Y),dtype=tf.float32))\n",
    "\n",
    "#6.Launch graph\n",
    "with tf.Session() as sess:\n",
    "    #6-1 initializer\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(10001):\n",
    "        cost_val,_=sess.run([cost,train],feed_dict={X:x_data,Y:y_data})\n",
    "        if step%200==0:\n",
    "            print(step,cost_val)\n",
    "\n",
    "    #7. accuracy report\n",
    "    h,c,a=sess.run([hypothesis,predicted,accuracy],feed_dict={X:x_data,Y:y_data})\n",
    "    print(\"\\n Hypothesis : \",h,\"\\n Correct (Y) :\",c,\"\\n Accuracy :\",a)\n",
    "\n",
    "\n"
   ]
  }
 ]
}