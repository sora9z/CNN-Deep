# 6.2 Softmax_zoo_classifier

## (1) 실습 목적
- 파일입력 sotfmax 구현 함수를 익힌다.
## (2) 사용한 API및 간단한 설명

- tf.nn.softmax_cross_entropy_with_logits
- tf.argmax
- tf.one_hot(indices, depth, on_value=None, off_value=None, axis=None, dtype=None, name=None)
  - 인덱스를 입력받아(indices) one-hot인코딩 결과를 리턴하는 one-hot 함수.
  - 입벽받은 Rank (N)보다 한 차원이 증가됨(N+1)
  - 기본 값으로 인덱스에 해당하는 위치에 1을, 그렇지 않으면0으로 표기.
  - on_value, off_value : 해당 위치에 on_value 값을 표기 그렇지 않으면 off_value로 표기함. 
  - ex) 
    - tf.one_hot([0,1,2],depth=3).eval(session=sess)</br>
  array([1.,0.,0],
        [0.,1.,0],
        [0.,0.,1])

## Softmax 동작
- cost_i = tf.nn.softmax_cross_entropy_with_logits(ligits=ligits,labels=Y_one_hot)
- tf.nn.softmax_cross_entropy_with_logits(labels, logits, axis=-1, name=None)
    - label :</br>
            Each vector along the class dimension should hold a valid probability distribution </br>
            e.g. for the case in which labels are of shape [batch_size, num_classes], each row of labels[i] must be a valid probability distribution.
    - logits: </br>
            Per-label activations, typically a linear output. These activation energies are interpreted as unnormalized log probabilities.
    - axis : The class dimension. Defaulted to -1 which is the last dimension.
  
    - logits=tf.matmul(X,W)+b 에서 바로 cost 함수를 얻는다.
    - hypothesis=tf.nn.softmax(logits) 은 사용하지않고 예츨할 때에만 사용된다.

- cost=tf.reduce_mean(cost_t)
    - 코스트 함수의 평균을 구한다.
- optimizer=tf.train.GradientDescentOptimizer(learning_rate=0.1)
    - 학습을 진행한다.
- prediction=tf.argmax(hypothesis,1)
    - 예측을 진행한다.
- correct_prediction=tf.equal(prediction,tf.argmax(Y_one_hot,1))
    - 정답과 예측을 비교한다.
- accuracy=tf.reduce_mean(tf.cast)


